{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant who gives precise and to the point answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_llm(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model = 'llama3.2',\n",
    "        messages = messages\n",
    "    )\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding inbrowser=True in launch() opens up a new browser window automatically\n",
    "# Adding share=True in launch() means that it can be accessed publically\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn = message_llm,\n",
    "    inputs = [gr.Textbox(label = \"Enter your message here:\", lines = 6)],\n",
    "    outputs = [gr.Textbox(label = \"Response:\", lines = 10)],\n",
    "    flagging_mode = \"never\"\n",
    ")\n",
    "view.launch(inbrowser=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the streaming output like ChatGPT in the gradio response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"ollama\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_system_message(tone):\n",
    "    return f\"You are a helpful assistant that gives precise outputs in markdown based on a {tone} tone\"\n",
    "\n",
    "def stream_llm(prompt, tone):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": set_system_message(tone)},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = \"llama3.2\",\n",
    "        messages = messages,\n",
    "        temperature = 0.7,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_llm,\n",
    "    inputs = [gr.Textbox(label=\"Your message: \"), gr.Dropdown([\"Formal\", \"Informal\", \"Professional\", \"Friendly\", \"Encouraging\", \"Humorous\", \"Sarcastic\"], label=\"Select tone\", value=\"Sarcastic\")],\n",
    "    outputs = [gr.Markdown(label=\"Response: \")],\n",
    "    flagging_mode = \"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a repeating assistant\"}] + history\n",
    "    # response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    reply = f\"{history[-1].get('content')}\" # response.choices[0].message.content\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with the assistant: \")\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Speak your question\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=chatbot\n",
    "    )\n",
    "\n",
    "    clear.click(lambda:None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
